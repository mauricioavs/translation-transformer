{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a0af73c",
      "metadata": {},
      "source": [
        "# Training an EN→ES Transformer (Europarl)\n",
        "A complete notebook: data loading, BPE tokenization, DataLoaders, training, and greedy decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb67685",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install -q datasets tokenizers sacrebleu sentencepiece tqdm torch torchvision torchaudio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "79afe8db",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "from transformer import Transformer\n",
        "from config import TransformerConfig\n",
        "from masks import create_padding_mask, create_decoder_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "8e97d4b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# General config\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "LR = 3e-4\n",
        "VOCAB_SIZE = 16000\n",
        "MAX_TRAIN_SAMPLES = 50000  # reduce para experimentos rápidos; usa None para todo el corpus\n",
        "VAL_FRACTION = 0.01\n",
        "tokenizer_path = Path(\"bpe_enes.json\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9c301f56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['translation'],\n",
            "        num_rows: 20091\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# 1) Load dataset\n",
        "raw_ds = load_dataset(\"Helsinki-NLP/europarl\", \"en-es\")\n",
        "train_valid = raw_ds[\"train\"].train_test_split(test_size=VAL_FRACTION, seed=SEED)\n",
        "\n",
        "if MAX_TRAIN_SAMPLES is not None:\n",
        "    max_n = min(MAX_TRAIN_SAMPLES, len(train_valid[\"train\"]))\n",
        "    train_valid[\"train\"] = train_valid[\"train\"].shuffle(seed=SEED).select(range(max_n))\n",
        "\n",
        "print(train_valid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0edbb57",
      "metadata": {},
      "source": [
        "## BPE Tokenizer\n",
        "We train (or load) a Byte-Pair Encoding tokenizer with the special tokens `<pad>`, `<s>`, `</s>`, `<unk>`. It is saved as `bpe_enes.json` for reuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fc365edc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 16000\n",
            "PAD/BOS/EOS: 0 1 2\n"
          ]
        }
      ],
      "source": [
        "SPECIAL_TOKENS = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
        "\n",
        "if tokenizer_path.exists():\n",
        "    tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "else:\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "    tokenizer.pre_tokenizer = ByteLevel()\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        min_frequency=2,\n",
        "        special_tokens=SPECIAL_TOKENS,\n",
        "    )\n",
        "\n",
        "    def text_iterator(ds):\n",
        "        for ex in ds:\n",
        "            yield ex[\"translation\"][\"en\"]\n",
        "            yield ex[\"translation\"][\"es\"]\n",
        "\n",
        "    tokenizer.train_from_iterator(text_iterator(train_valid[\"train\"]), trainer=trainer)\n",
        "    tokenizer.decoder = ByteLevelDecoder()\n",
        "    tokenizer.post_processor = TemplateProcessing(\n",
        "        single=\"<s> $A </s>\",\n",
        "        pair=\"<s> $A </s> </s> $B </s>\",\n",
        "        special_tokens=[(\"<s>\", tokenizer.token_to_id(\"<s>\")), (\"</s>\", tokenizer.token_to_id(\"</s>\"))],\n",
        "    )\n",
        "    tokenizer.save(str(tokenizer_path))\n",
        "\n",
        "# Aseguramos decoder/post-procesador también cuando cargamos el fichero\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<s> $A </s>\",\n",
        "    pair=\"<s> $A </s> </s> $B </s>\",\n",
        "    special_tokens=[(\"<s>\", tokenizer.token_to_id(\"<s>\")), (\"</s>\", tokenizer.token_to_id(\"</s>\"))],\n",
        ")\n",
        "\n",
        "PAD_ID = tokenizer.token_to_id(\"<pad>\")\n",
        "BOS_ID = tokenizer.token_to_id(\"<s>\")\n",
        "EOS_ID = tokenizer.token_to_id(\"</s>\")\n",
        "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
        "\n",
        "print(\"Vocab size:\", VOCAB_SIZE)\n",
        "print(\"PAD/BOS/EOS:\", PAD_ID, BOS_ID, EOS_ID)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f626b1",
      "metadata": {},
      "source": [
        "## Preprocessing and DataLoaders\n",
        "We encode each EN→ES sentence pair into IDs, trim sequences to `MAX_LEN`, and construct tensors with dynamic padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c51e768e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['src', 'tgt_in', 'tgt_out'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['src', 'tgt_in', 'tgt_out'],\n",
            "        num_rows: 20091\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "def encode_example(example):\n",
        "    en_text = example[\"translation\"][\"en\"]\n",
        "    es_text = example[\"translation\"][\"es\"]\n",
        "\n",
        "    src_ids = tokenizer.encode(en_text).ids[: MAX_LEN - 1] + [EOS_ID]\n",
        "    tgt_ids = tokenizer.encode(es_text).ids[: MAX_LEN - 2]\n",
        "    tgt_in = [BOS_ID] + tgt_ids\n",
        "    tgt_out = tgt_ids + [EOS_ID]\n",
        "    return {\n",
        "        \"src\": src_ids,\n",
        "        \"tgt_in\": tgt_in,\n",
        "        \"tgt_out\": tgt_out,\n",
        "    }\n",
        "\n",
        "processed = train_valid.map(\n",
        "    encode_example,\n",
        "    remove_columns=[\"translation\"],\n",
        "    num_proc=4,\n",
        ").with_format(\"python\")\n",
        "\n",
        "print(processed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ccada9a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src torch.Size([64, 79])\n",
            "tgt_in torch.Size([64, 86])\n",
            "tgt_out torch.Size([64, 86])\n"
          ]
        }
      ],
      "source": [
        "def pad_sequences(seqs, pad_id=PAD_ID):\n",
        "    max_len = max(len(s) for s in seqs)\n",
        "    return torch.tensor([s + [pad_id] * (max_len - len(s)) for s in seqs], dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"src\": pad_sequences([b[\"src\"] for b in batch]),\n",
        "        \"tgt_in\": pad_sequences([b[\"tgt_in\"] for b in batch]),\n",
        "        \"tgt_out\": pad_sequences([b[\"tgt_out\"] for b in batch]),\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    processed[\"train\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    processed[\"test\"],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "for key, tensor in batch.items():\n",
        "    print(key, tensor.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c392e4a",
      "metadata": {},
      "source": [
        "## Transformer Model\n",
        "\n",
        "We instantiate the transformer defined in this repository and configure the optimizer and loss function with ignore_index=PAD_ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "04cd7f84",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = TransformerConfig(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    d_model=256,\n",
        "    n_heads=4,\n",
        "    num_encoder_layers=4,\n",
        "    num_decoder_layers=4,\n",
        "    d_ff=1024,\n",
        "    dropout=0.1,\n",
        "    pad_id=PAD_ID,\n",
        "    max_len=MAX_LEN,\n",
        ")\n",
        "\n",
        "model = Transformer(config).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98))\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e10232",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Simple training/validation loop with gradient clipping and checkpoint saving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "b74ff0da",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss=3.3602 val_loss=3.4819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train_loss=3.3129 val_loss=3.3622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: train_loss=3.1739 val_loss=3.3077\n",
            "Modelo guardado en transformer_europarl.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    steps = 0\n",
        "    for batch in tqdm(loader, leave=False):\n",
        "        src = batch[\"src\"].to(device)\n",
        "        tgt_in = batch[\"tgt_in\"].to(device)\n",
        "        tgt_out = batch[\"tgt_out\"].to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(src, tgt_in)\n",
        "            loss = criterion(logits.view(-1, VOCAB_SIZE), tgt_out.view(-1))\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "\n",
        "    return total_loss / max(steps, 1)\n",
        "\n",
        "train_history, val_history = [], []\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = run_epoch(train_loader, train=True)\n",
        "    val_loss = run_epoch(val_loader, train=False)\n",
        "    train_history.append(train_loss)\n",
        "    val_history.append(val_loss)\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"transformer_europarl.pt\")\n",
        "print(\"Modelo guardado en transformer_europarl.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07f7d81",
      "metadata": {},
      "source": [
        "## Greedy decoding\n",
        "Step-by-step translation using the encoder+decoder part of the current model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b3357f42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EN: That is what I wanted to say in conclusion.\n",
            "ES pred: Eso es lo que he dicho.\n",
            "\n",
            "EN: End of quote.\n",
            "ES pred: Las empresas.\n",
            "\n",
            "EN: Young people are unquestionably a key resource in whom we must invest in order to revitalise the European Union economy as a whole.\n",
            "ES pred: Los ciudadanos de la Unión Europea deben ser un gran número de personas.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"transformer_europarl_1.pt\"))\n",
        "\n",
        "def translate_sentence(model, text, max_new_tokens=MAX_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_ids = tokenizer.encode(text).ids[: MAX_LEN - 1] + [EOS_ID]\n",
        "        src = torch.tensor(src_ids, device=device).unsqueeze(0)\n",
        "        src_mask = create_padding_mask(src, PAD_ID)\n",
        "        memory = model.encode(src, src_mask)\n",
        "\n",
        "        ys = torch.tensor([[BOS_ID]], device=device)\n",
        "        for step in range(max_new_tokens):\n",
        "            tgt_mask = create_decoder_mask(ys, PAD_ID, device=device)\n",
        "            out = model.decode(ys, memory, tgt_mask, src_mask)\n",
        "            logits = model.generator(out[:, -1])\n",
        "            # Evita que el modelo genere PAD/BOS como token siguiente;\n",
        "            # y fuerza al menos 1-2 tokens antes de permitir EOS.\n",
        "            logits[:, [PAD_ID, BOS_ID]] = -1e9\n",
        "            if step < 1:\n",
        "                logits[:, EOS_ID] = -1e9\n",
        "            next_id = int(logits.argmax(dim=-1).item())\n",
        "            ys = torch.cat([ys, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "            if next_id == EOS_ID:\n",
        "                break\n",
        "\n",
        "    decoded_ids = [i for i in ys[0, 1:].tolist() if i not in {PAD_ID, BOS_ID, EOS_ID}]\n",
        "    if decoded_ids:\n",
        "        return tokenizer.decode(decoded_ids, skip_special_tokens=True).strip()\n",
        "    # Fallback: decodifica todo salvo BOS y PAD por si solo hubo EOS temprano\n",
        "    fallback_ids = [i for i in ys[0, 1:].tolist() if i not in {PAD_ID, BOS_ID}]\n",
        "    return tokenizer.decode(fallback_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "samples = [train_valid[\"test\"][i][\"translation\"][\"en\"] for i in range(3)]\n",
        "for s in samples:\n",
        "    print(\"EN:\", s)\n",
        "    pred = translate_sentence(model, s)\n",
        "    print(\"ES pred:\", pred if pred else \"<vacío>\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "28c285fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EN: That is what I wanted to say in conclusion.\n",
            "ES pred: Eso es lo que me parece.\n",
            "\n",
            "EN: End of quote.\n",
            "ES pred: Las empresas.\n",
            "\n",
            "EN: Young people are unquestionably a key resource in whom we must invest in order to revitalise the European Union economy as a whole.\n",
            "ES pred: La Unión Europea debe ser una economía europea en el ámbito de la economía.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"transformer_europarl_2.pt\"))\n",
        "\n",
        "def translate_sentence(model, text, max_new_tokens=MAX_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_ids = tokenizer.encode(text).ids[: MAX_LEN - 1] + [EOS_ID]\n",
        "        src = torch.tensor(src_ids, device=device).unsqueeze(0)\n",
        "        src_mask = create_padding_mask(src, PAD_ID)\n",
        "        memory = model.encode(src, src_mask)\n",
        "\n",
        "        ys = torch.tensor([[BOS_ID]], device=device)\n",
        "        for step in range(max_new_tokens):\n",
        "            tgt_mask = create_decoder_mask(ys, PAD_ID, device=device)\n",
        "            out = model.decode(ys, memory, tgt_mask, src_mask)\n",
        "            logits = model.generator(out[:, -1])\n",
        "            # Evita que el modelo genere PAD/BOS como token siguiente;\n",
        "            # y fuerza al menos 1-2 tokens antes de permitir EOS.\n",
        "            logits[:, [PAD_ID, BOS_ID]] = -1e9\n",
        "            if step < 1:\n",
        "                logits[:, EOS_ID] = -1e9\n",
        "            next_id = int(logits.argmax(dim=-1).item())\n",
        "            ys = torch.cat([ys, torch.tensor([[next_id]], device=device)], dim=1)\n",
        "            if next_id == EOS_ID:\n",
        "                break\n",
        "\n",
        "    decoded_ids = [i for i in ys[0, 1:].tolist() if i not in {PAD_ID, BOS_ID, EOS_ID}]\n",
        "    if decoded_ids:\n",
        "        return tokenizer.decode(decoded_ids, skip_special_tokens=True).strip()\n",
        "    # Fallback: decodifica todo salvo BOS y PAD por si solo hubo EOS temprano\n",
        "    fallback_ids = [i for i in ys[0, 1:].tolist() if i not in {PAD_ID, BOS_ID}]\n",
        "    return tokenizer.decode(fallback_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "samples = [train_valid[\"test\"][i][\"translation\"][\"en\"] for i in range(3)]\n",
        "for s in samples:\n",
        "    print(\"EN:\", s)\n",
        "    pred = translate_sentence(model, s)\n",
        "    print(\"ES pred:\", pred if pred else \"<vacío>\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779f2696",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
